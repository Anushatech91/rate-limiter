So, we have set a constant timer of 10 seconds and we are setting users to 1 and the number of iterations is 6. So, over a period of 60 seconds approximately there would be 6 requests sent in a period of 10 seconds each. So, if we see the results, right, so that's a ready state log. So, the LLM is not generating a fixed response but yeah the response is variable but we can see the amount of first tokens used. So, for the first dummy user 2, it has used 188 tokens. For the second user 1, dummy user 1, like it has used 214 and the dummy user 3, it has used 236 tokens after the first request. So, if we see here only 3 requests are logged because the rest of the requests are denied because it has reached the max tokens limit, okay. So, when the next request comes after this, for the second, I mean dummy user 1 and dummy user 3, because the max tokens, I mean burst tokens consumed are like even higher than 200, 214 and 236. In the next 10 seconds, only 10 tokens will be replenished. So, the max tokens also wouldn't be even within positive. And for the first case, even though the max tokens are in check but because the burst capacity is 150, so the second request also like basically wouldn't be successful because it is 188. So, after 10 seconds also, it wouldn't be coming within inside the burst capacity, right. So, that is the reason it would also be denied.
